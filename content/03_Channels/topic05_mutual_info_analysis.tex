% content/03_Channels/topic05_mutual_info_analysis.tex

\begin{intuitionbox}{1. 互信息的物理直观：不确定性的消除}
    \textbf{核心概念}：信息量 = 不确定度的减小量。
    \begin{itemize}
        \item \textbf{发送前}：对发送端 $X$ 的不确定度是 $H(X)$。
        \item \textbf{接收后}：收到符号 $y$ 后，对 $X$ 依然存在的不确定度是 $H(X|Y)$（条件熵）。
        \item \textbf{获得的信息}：
        \[
            \begin{aligned}
            I(X;Y) &= \text{原本不确定度} - \text{剩余不确定度} \\
                   &= H(X) - H(X|Y)
            \end{aligned}
        \]
    \end{itemize}
\end{intuitionbox}

\begin{kbox}{2. 条件期望视角的详细推导 (Handwritten Note)}
    % [修改] 使用 \small 缩小字号以防止公式溢出
    \small
    此部分采用\textbf{分层理解}的方法，解释 $E[I(x_i|y_j)]$ 是如何推导为 $H(X|Y)$ 的。
    
    \tcbline
    
    \textbf{第一步：理解期望的层次}
    
    $E(\cdot)$ 是求期望。式子含义是在 $Y=y_j$ 发生的概率下，求 $X=x_i$ 的相关期望。可以理解为：\textbf{首先假设 $Y=y_j$ 其实是确定的}。
    
    \textbf{第二步：构建“方框”结构}
    
    整个期望可以拆解为外层对 $Y$ 的加权和内层对 $X$ 的计算：
    
    \[ \sum_{j} P(Y=y_j) \times \tikzmarknode{boxnode}{\framebox[1.2em]{\vphantom{M}}} \]
    
    \begin{itemize}
        \item \textbf{方框 $\square$ 中的内容}：
        这是在已知 $Y=y_j$ (确定) 的情况下，关于 $X$ 的信息量期望。
        \[ \text{方框内容} = \sum_{i} P(x_i|y_j) \cdot \log_2 \frac{1}{P(x_i|y_j)} \]
        这实际上就是\textbf{特定条件下的熵} $H(X|Y=y_j)$。
    \end{itemize}

    \begin{center}
    \begin{tikzpicture}[>=Stealth, node distance=1.0cm]
        % 绘制结构图
        \node (outer) [draw=mainblue, fill=mainblue!5, rounded corners, inner sep=6pt, font=\scriptsize] {
            $\displaystyle \sum_{j} P(y_j) \times \left[ \mathbf{\dots} \right]$
        };
        
        \node (inner) [draw=alertred, fill=alertred!5, rounded corners, below=of outer, align=center, font=\scriptsize] {
            \textbf{内部方框 (固定 $y_j$)} \\
            $\displaystyle \sum_{i} P(x_i|y_j) \log_2 \frac{1}{P(x_i|y_j)}$ \\
            $= H(X|Y=y_j)$
        };
        
        \draw[->, thick, mainblue] (outer) -- node[right, font=\tiny] {代入方框} (inner);
    \end{tikzpicture}
    \end{center}

    \textbf{第三步：整合得出结论}
    
    将方框内容代回原式，即可得：
    \[
    \begin{aligned}
        E[I] &= \sum_{j} P(y_j) \left[ \sum_{i} P(x_i|y_j) \log_2 \frac{1}{P(x_i|y_j)} \right] \\
             &= \sum_{j} P(y_j) \cdot H(X|Y=y_j) \\
             &= H(X|Y) \quad (\text{平均条件熵})
    \end{aligned}
    \]
\end{kbox}

\begin{kbox}{3. 严谨推导：联合概率法 (Derivation)}
    % [修改] 使用 \small 缩小字号，防止长公式溢出
    \small
    \textit{这是前文基于联合概率定义的另一种推导路径。}
    
    \[ I(X;Y) = E[I(x_i)] - E[I(x_i|y_j)] \]
    
    利用数学期望定义（双重求和，权重为联合概率）：
    % [修改] 强制换行并调整间距
    \[
    \begin{aligned}
        &E[I(x_i|y_j)] \\
        &= \sum_{j=1}^m \sum_{i=1}^n \underbrace{P(x_i, y_j)}_{\text{联合概率}} \log_2 \frac{1}{P(x_i|y_j)} \\
        &\text{代入贝叶斯} \ P(x_i, y_j) = P(y_j)P(x_i|y_j): \\
        &= \sum_{j=1}^m P(y_j) \underbrace{\left[ \sum_{i=1}^n P(x_i|y_j) \log_2 \frac{1}{P(x_i|y_j)} \right]}_{H(X|Y=y_j)} \\
        &= H(X|Y)
    \end{aligned}
    \]
    \textbf{最终结论}：$I(X;Y) = H(X) - H(X|Y)$
\end{kbox}

\begin{kbox}{4. 互信息的对称性 (Symmetry)}
    \[
    \begin{aligned}
        I(X;Y) &= H(X) - H(X|Y) \quad (\text{发送} - \text{损失}) \\
               &= H(Y) - H(Y|X) \quad (\text{接收} - \text{噪声})
    \end{aligned}
    \]
\end{kbox}